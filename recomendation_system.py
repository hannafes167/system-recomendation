# -*- coding: utf-8 -*-
"""recomendation-system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DY3TkCgHMsqxbOzr0WBNp4XJNXmbS0J7
"""

!pip install gdown

"""## Import dataset

Dataset proyek ini berasal dari [Goodbooks-10k](https://github.com/zygmuntz/goodbooks-10k). Dataset tersebut diunggah ke Google Drive dan diakses menggunakan `gdown`. File dalam format `.zip` berisi beberapa file CSV seperti `books.csv`, `ratings.csv`, dll.

"""

import gdown
import zipfile
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import csr_matrix
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from sklearn.metrics import precision_score

file_id = '1--AgpACCsPpHzYkRxIyzZ8-tEDaY5bnL'
output = 'goodbooks.zip'
gdown.download(f'https://drive.google.com/uc?id={file_id}', output, quiet=False)

with zipfile.ZipFile(output, 'r') as zip_ref:
    zip_ref.extractall('goodbooks')

"""# Data Loading
Mengambil data dari folder goodbooks, menggunakan file books dan rating untuk pengolahan sistem rekomendasi.

"""

# Load data
books = pd.read_csv('goodbooks/books.csv')
ratings = pd.read_csv('goodbooks/ratings.csv')

print("Books Dataset:")
print(books.head())

print("\nRatings Dataset:")
print(ratings.head())

"""**Insights:**
- Memuat data dari dua file: books.csv dan ratings.csv.

# Exploratory Data Analysis (EDA)
Pada tahap ini dilakukan pengecekan kondisi data.
Pada tahap ini, dilakukan beberapa analisis awal terhadap data:

- Menampilkan struktur data (jumlah baris, kolom, tipe data).
- Menampilkan statistik deskriptif untuk memahami rentang nilai pada dataset.
- Mengecek apakah ada missing value.
- Menghitung jumlah user unik dan jumlah buku unik yang diberikan rating.
"""

# Cek struktur dataset books
print("Informasi dataset books:")
print(books.info())
print("\nStatistik deskriptif books:")
print(books.describe())

# Cek struktur dataset ratings
print("\nInformasi dataset ratings:")
print(ratings.info())
print("\nStatistik deskriptif ratings:")
print(ratings.describe())

# Cek missing value
print("\nMissing value pada books:")
print(books.isnull().sum())

print("\nMissing value pada ratings:")
print(ratings.isnull().sum())

# Cek jumlah user dan book unik
print("\nJumlah user unik:", ratings['user_id'].nunique())
print("Jumlah buku unik yang dirating:", ratings['book_id'].nunique())

"""Pada tahap ini dilakukan analisis eksploratif terhadap data untuk memahami karakteristik distribusi rating

"""

plt.figure(figsize=(8,5))
sns.countplot(x='rating', data=ratings, palette='viridis')
plt.title('Distribusi Rating Buku')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

"""Visualisasi buku yang paling populer"""

# Menggabungkan ratings dan books untuk mendapatkan nama buku
ratings_with_titles = ratings.merge(books[['book_id', 'title']], on='book_id')
top_books = ratings_with_titles['title'].value_counts().head(10)

plt.figure(figsize=(10,6))
sns.barplot(x=top_books.values, y=top_books.index, palette='rocket')
plt.title('Top 10 Buku dengan Rating Terbanyak')
plt.xlabel('Jumlah Rating')
plt.ylabel('Judul Buku')
plt.show()

"""Visualisai user yang memberikan rating terbanyak"""

# Top 10 user yang memberikan rating terbanyak
top_users = ratings['user_id'].value_counts().head(10)

plt.figure(figsize=(10,6))
sns.barplot(x=top_users.values, y=top_users.index, palette='mako')
plt.title('Top 10 User yang Memberikan Rating Terbanyak')
plt.xlabel('Jumlah Rating Diberikan')
plt.ylabel('User ID')
plt.show()

"""**Insights:**
- Dataset books memiliki 10.000 baris dan 23 kolom.
- Dataset ratings memiliki 981.756 baris dan 3 kolom
- Terdapat missing values pada kolom:
  - isbn: 700
  - isbn13: 585
  - original_publication_year: 21
  - original_title: 585
  - language_code: 1.084
-Jumlah pengguna unik: 53.424
- Jumlah buku unik yang diberi rating: 10.000
- Visualisasi:
  - Distribusi nilai rating yang diberikan pengguna menunjukkan kecenderungan rating tinggi atau rendah. Yang terbanyak adalah rating 4 dan terendah rating 1.
  - Buku dengan jumlah rating terbanyak, yang bisa menjadi kandidat populer untuk rekomendasi. Top 10 buku dengan jumlah rating terbanyak adalah The End of Poverty, Harry Potter and the Half-Blood Prince (Harry Potter, #6), Harry Potter and the Order of the Phoenix (Harry Potter, #5), Burmese Days, Galapagos, The Lover, The Potrait of a Lady, Tropic of Cancer, I am Chariotte Simmons.
  - Penguna yang paling aktif memberi rating, penting untuk analisis perilaku dan filtering strategi. Pengguna paling aktif dengan memberikan rating sebanyak 199.

## Data Preparation

Tahap preprocessing bertujuan untuk membersihkan dan mempersiapkan data sebelum digunakan untuk membangun model sistem rekomendasi.

1. Mengecek missing value dan membiarkan datanya karena tidak akan dipakai lebih lanjut kolom tersebut
"""

# Cek missing value pada books
print("Missing value pada books:")
print(books.isnull().sum())

# Cek missing value pada ratings
print("\nMissing value pada ratings:")
print(ratings.isnull().sum())

"""2. Mengambil kolom book_id, title, authors dan average_ratings pada data books dan mengambil seluruh kolom pada data ratings yaitu book_id, user_id, dan rating."""

# Hanya ambil kolom penting dari books
books_cleaned = books[['book_id', 'title', 'authors', 'average_rating']]

# Pastikan ratings tetap dengan 3 kolom: book_id, user_id, rating
ratings_cleaned = ratings.copy()

"""3. Melakukan filtering dengan mengambil buku yang mempunyai minimal 50 rating"""

# Filter buku yang mendapatkan setidaknya 50 rating
book_counts = ratings_cleaned['book_id'].value_counts()
popular_books = book_counts[book_counts >= 50].index
ratings_filtered = ratings_cleaned[ratings_cleaned['book_id'].isin(popular_books)]

# Filter user yang memberikan setidaknya 50 rating
user_counts = ratings_filtered['user_id'].value_counts()
active_users = user_counts[user_counts >= 50].index
ratings_filtered = ratings_filtered[ratings_filtered['user_id'].isin(active_users)]

print(f"Jumlah data setelah filtering: {ratings_filtered.shape}")

"""4. Melakukan merge books dan ratings dengan nama data ratings_final"""

# Merge ratings_filtered dengan books_cleaned
ratings_final = ratings_filtered.merge(books_cleaned, on='book_id')

ratings_final.head()

"""5. Persiapan untuk Content-Based Filtering (CBF) menggunakan kombinasi kolom `title` dan `authors` dari buku sebagai fitur deskripsi konten.
Akan digunakan TF-IDF (Term Frequency-Inverse Document Frequency) untuk mengubah teks menjadi vektor numerik. Serta menngisi jika terdapat nilai kosong.
"""

books_cleaned = books_cleaned.drop_duplicates(subset='title')

# Gabungkan fitur relevan
books_cleaned['combined_features'] = (
    books_cleaned['title'].fillna('') + ' ' +
    books_cleaned['authors'].fillna('') + ' '
)

# TF-IDF dengan ngram
tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))
tfidf_matrix = tfidf.fit_transform(books_cleaned['combined_features'])

print(f"TF-IDF matrix shape: {tfidf_matrix.shape}")
indices = pd.Series(books_cleaned.index, index=books_cleaned['title']).drop_duplicates()

"""6. Persiapan untuk Collaborative Filtering (CF) menggunakan user-item-matrix untuk mencari pola kemiripan antar pengguna.

"""

# Buat pivot table user-item
user_item_matrix = ratings_final.pivot_table(index='user_id', columns='book_id', values='rating')

# Ubah menjadi sparse matrix (optional untuk efisiensi memori)
user_item_sparse = csr_matrix(user_item_matrix.fillna(0))

print(f"User-Item matrix shape: {user_item_matrix.shape}")

"""**Insights:**

- Missing values pada kolom isbn, isbn13, original_publication_year, original_title, dan language_code dibiarkan, karena kolom-kolom tersebut tidak digunakan dalam pemodelan.
- Kolom yang digunakan:
  - Dari dataset books: book_id, title, authors, dan average_rating.
  - Dari dataset ratings: seluruh kolom digunakan.
- Filtering:
  - Hanya buku dengan minimal 50 rating yang digunakan.
  - Hanya pengguna yang memberikan minimal 50 rating yang disertakan.
  - Tujuan: mengurangi sparsity pada matriks user-item.
  - Jumlah data setelah filtering: 421.012 baris.
- Dilakukan penggabungan data ratings dan books berdasarkan book_id untuk melengkapi informasi judul buku yang akan digunakan dalam Content-Based Filtering (CBF).
- Ukuran matriks TF-IDF (judul + penulis): (34.301, 2.411)
- Ukuran matriks user-item (pivot): (4.911, 805)

## Modeling

Pada metode Content-Based Filtering, rekomendasi dibuat berdasarkan kemiripan fitur konten (dalam hal ini, `title` dan `authors`) antar buku.

1. CBF menggunakan Cosine Similarity untuk mengukur tingkat kemiripan antar buku.
"""

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

print(f"Cosine similarity matrix shape: {cosine_sim.shape}")

"""2. CF menggunakan pivot tabel dan memakai model KNN
Pada metode Collaborative Filtering, rekomendasi diberikan berdasarkan kesamaan perilaku pengguna terhadap item (buku), tanpa melihat konten buku itu sendiri.
"""

user_book_matrix = ratings_final.pivot_table(index='user_id', columns='title', values='rating')

user_book_matrix = user_book_matrix.fillna(0)

print(user_book_matrix.shape)
user_book_matrix.head()

# Membuat model KNN
model_knn = NearestNeighbors(metric='cosine', algorithm='brute')
model_knn.fit(user_book_matrix)

"""1. Train Content-Based Filtering (CBF)"""

# Mapping
def recommend_books_cbf(title, cosine_sim=cosine_sim):
    if title not in indices:
        print("Judul buku tidak ditemukan.")
        return []

    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:6]
    book_indices = [i[0] for i in sim_scores]

    return books_cleaned['title'].iloc[book_indices].tolist()

recommend_books_cbf('The Woman in the Dunes')

"""2. Train model collaborative filtering (CF)"""

def recommend_books_user(user_id, model=model_knn, user_item_matrix=user_book_matrix, n_recommendations=5):
    if user_id not in user_item_matrix.index:
        print("User ID tidak ditemukan.")
        return []

    distances, indices = model.kneighbors(user_item_matrix.loc[[user_id]], n_neighbors=n_recommendations+1)

    similar_users = user_item_matrix.index[indices.flatten()][1:]

    recommended_books = []

    for sim_user in similar_users:
        books = user_item_matrix.loc[sim_user]
        books = books[books > 0].index.tolist()
        recommended_books.extend(books)

    recommended_books = list(set(recommended_books))

    return recommended_books[:n_recommendations]

recommend_books_user(23637)

"""**Insights:**
- Content-Based Filtering (CBF) menggunakan cosine similarity terhadap fitur gabungan title dan authors. Hasil rekomendasi model CBF untuk buku The Women in the Dunes:
['Me and Earl and the Dying Girl', 'The Jungle',
 'The Fault in Our Stars', 'Paper Towns', 'Looking for Alaska']
Buku-buku ini memiliki tema emosional, psikologis, atau eksistensial yang relevan.
- Collaborative Filtering (CF) menggunakan matriks pivot dan model K-Nearest Neighbors (KNN) untuk menemukan kemiripan antar pengguna. Hasil rekomendasi model CF untuk pengguna ID 23637:
['Islands in the Stream', 'Boy: Tales of Childhood',
 'The Path Between the Seas', 'A Briefer History of Time',
 'Play It as It Lays']
Rekomendasi ini mencerminkan minat pengguna terhadap sejarah, biografi, dan sains populer.

## Evaluation
Pada tahap evaluasi, dilakukan pengukuran performa model rekomendasi untuk dua pendekatan: Collaborative Filtering (CF) dan Content-Based Filtering (CBF). Masing-masing pendekatan dievaluasi dengan metode yang sesuai dengan karakteristik model dan data.

1. Collaborative Filtering (CF) Model CF dievaluasi menggunakan metrik Precision@5
"""

user_id = 23637
user_data = ratings_final[ratings_final['user_id'] == user_id]

relevant_books = user_data[user_data['rating'] >= 4]['title'].tolist()

recommended_books = ['Islands in the Stream', 'Boy: Tales of Childhood',
                     'The Path Between the Seas', 'A Briefer History of Time',
                     'Play It as It Lays']

k = 5
y_true = [1 if book in relevant_books else 0 for book in recommended_books[:k]]
y_pred = [1] * len(y_true)

from sklearn.metrics import precision_score
precision_at_k = precision_score(y_true, y_pred)

print(f"Precision@{k}: {precision_at_k:.2f}")

"""2. Content-Based Filtering (CBF) Berbeda dengan CF, model CBF tidak dapat dievaluasi secara kuantitatif karena tidak ada label eksplisit (rating) yang menunjukkan relevansi item. Oleh karena itu, evaluasi dilakukan secara kualitatif dengan membandingkan kesamaan konten antar buku.

Model CBF memberikan rekomendasi berikut untuk buku “The Women in the Dunes”:

- Me and Earl and the Dying Girl
- The Jungle
- The Fault in Our Stars
- Paper Towns
- Looking for Alaska

**Insights:**
- Model Collaborative Filtering menghasilkan Precision@5 sebesar 0.40. Artinya, 2 dari 5 buku yang direkomendasikan sesuai dengan preferensi user (berdasarkan rating ≥ 4).
- Hasil model CBF maka buku-buku tersebut memiliki tema yang sejalan, yaitu psikologis, emosional, dan eksistensial. Ini menunjukkan bahwa model mampu menangkap fitur tematik dan narati dari buku input.
"""